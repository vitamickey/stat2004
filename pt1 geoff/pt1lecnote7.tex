\chapter[Lec 7: Distribution of s squared]{Lec 7: Distribution of \(s^2\)}\label{ch:Lec 7:dist of s squared}

In essence, there is one key takeaway to get from this lecture note.

\medskip 

Let \(X_1,\ldots,X_n \overset{\text{i.i.d.}}{\sim} N(\mu,\sigma^2)\), where \(\mu, \sigma^2\) are both unknown. 
Geoff provides a pretty comprehensive outline of the derivation of the following key result. 

\begin{theorem}[Distribution of sample variance]\label{thm:dist of sample var}
    Let \(s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(X_i - \bar{X})}^2\). Then, 
    \[\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^{2}.\]
\end{theorem}

In its most basic form, I prefer to think about the above result as follows. 

\begin{theorem}[Distribution of sum of squares]\label{thm:dist of sum of squares}
    \[\frac{1}{\sigma^2}\sum_{i=1}^{n}{(X_i - \bar{X})}^2 \sim \chi_{n-1}^{2}.\]
\end{theorem}

(Introduce this)
It is easy to find that the \textbf{biased MLE} for \(\sigma^2\) is given by 

\begin{equation}\label{eq:biased MLE var}
    \hat{\sigma^2} = \frac{1}{n}\sum_{i=1}^{n}{(X_i - \bar{X})}^2.
\end{equation}

(Insert stuff about its relation to \cref{thm:dist of sum of squares}).
We also know that sample variance, which is equivalent to the \textbf{bias-corrected MLE}, or \textbf{unbiased MLE}, is given by 

\begin{equation}\label{eq:unbiased MLE var}
    s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(X_i - \bar{X})}^2.
\end{equation}

Combining this with \cref{thm:dist of sum of squares}
From this, we can use helpful properties of the \(\chi^2\) distribution when needing to calculate some results. 
(to be continued)